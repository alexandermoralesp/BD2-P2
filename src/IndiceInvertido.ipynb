{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IndiceInvertido.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "nltk.download(\"punkt\")\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "import os.path\n",
        "import math\n",
        "import json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r0JmdeMesg0G",
        "outputId": "2a04441e-5e42-45ee-e4eb-894e4cedfdf6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CARGAR CSV"
      ],
      "metadata": {
        "id": "7CKY3jr32npE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df1 = pd.read_csv(\"/content/drive/MyDrive/news/articles1.csv\").drop([\"Unnamed: 0\"],axis=1)\n",
        "df2 = pd.read_csv(\"/content/drive/MyDrive/news/articles2.csv\").drop([\"Unnamed: 0\"],axis=1)\n",
        "df3 = pd.read_csv(\"/content/drive/MyDrive/news/articles3.csv\").drop([\"Unnamed: 0\"],axis=1)\n",
        "news = pd.concat([df1,df2,df3], ignore_index=True)"
      ],
      "metadata": {
        "id": "yGJaJC8dsq-m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = news\n",
        "df['title_content'] = df.title + \" \" + df.content"
      ],
      "metadata": {
        "id": "2Xq6XftI15Wi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CLASE INDICE"
      ],
      "metadata": {
        "id": "RRbMvBLx2sD3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class indice_invertido:\n",
        "  def __init__(self, archivo_indice):\n",
        "    self.archivo_indice = archivo_indice\n",
        "    self.stemmer = SnowballStemmer(\"spanish\")\n",
        "    self.index = {}\n",
        "    self.idf = {}\n",
        "    self.tf = {}\n",
        "    self.tf_idf = {}\n",
        "    self.length = {}\n",
        "  \n",
        "  def procesamiento(self, texto):\n",
        "    texto_tokens = nltk.word_tokenize(texto)\n",
        "\n",
        "    with(open(\"/content/drive/MyDrive/news/stoplist.txt\")) as file:\n",
        "      stoplist = [line.lower().strip() for line in file]\n",
        "    stoplist += [',', '.', '?', 'Â¿', \":\", \"``\", \"''\", \"(\", \")\", \":\", \";\"]\n",
        "\n",
        "    texto_tokens_c = texto_tokens[ : ]\n",
        "    for token in texto_tokens:\n",
        "      if token.lower() in stoplist:\n",
        "        texto_tokens_c.remove(token)\n",
        "\n",
        "    texto_tokens_s = []\n",
        "    for w in texto_tokens_c:\n",
        "      texto_tokens_s.append(self.stemmer.stem(w))\n",
        "    return texto_tokens_s\n",
        "  \n",
        "  def building(self, df):\n",
        "    for i in range(df.shape[0]):\n",
        "      texto_filtrado = self.procesamiento(df.title_content[i])\n",
        "      for w in texto_filtrado:\n",
        "        if w in self.index:\n",
        "          self.index[w] = self.index[w] + [i] \n",
        "          self.tf[w][i] += 1\n",
        "        else:\n",
        "          self.index[w] = [i]\n",
        "          self.tf[w] = {}\n",
        "          for reg_i in range(df.shape[0]):\n",
        "            self.tf[w][reg_i] = 0\n",
        "          self.tf[w][reg_i] = 1\n",
        "    \n",
        "\n",
        "\n",
        "    for key in self.index:\n",
        "      self.index[key] = sorted(list(set(self.index[key])))\n",
        "    # compute the idf\n",
        "      self.idf[key] = math.log10(df.shape[0]/len(self.index[key]))\n",
        "    for key in self.tf:\n",
        "      self.tf_idf[key] = {}\n",
        "      for key2 in self.tf[key]:\n",
        "        self.tf_idf[key][key2] = math.log10(1+self.tf[key][key2])*self.idf[key]\n",
        "            \n",
        "    # compute the length (norm)\n",
        "    for tx in range(df.shape[0]):\n",
        "      self.length[tx] = 0\n",
        "      for key in self.tf:\n",
        "        if self.tf[key][tx] != 0:\n",
        "          self.length[tx] += (math.log10(self.tf[key][tx]))**2\n",
        "      self.length[tx] = (self.length[tx])**0.5\n",
        "\n",
        "    # store in disk\n",
        "    out_file = open(self.archivo_indice+\"_index.json\", \"w\")\n",
        "    json.dump(self.index, out_file, indent = 6)  \n",
        "    out_file.close()\n",
        "    out_file = open(self.archivo_indice+\"_idf.json\", \"w\")\n",
        "    json.dump(self.idf, out_file, indent = 6)  \n",
        "    out_file.close()\n",
        "    out_file = open(self.archivo_indice+\"_tf.json\", \"w\")\n",
        "    json.dump(self.tf, out_file, indent = 6)  \n",
        "    out_file.close()\n",
        "    out_file = open(self.archivo_indice+\"_tf_idf.json\", \"w\")\n",
        "    json.dump(self.tf_idf, out_file, indent = 6)  \n",
        "    out_file.close()\n",
        "    out_file = open(self.archivo_indice+\"_length.json\", \"w\")\n",
        "    json.dump(self.length, out_file, indent = 6)  \n",
        "    out_file.close()\n",
        "    pass\n",
        "\n",
        "  def get_tfidf(self, query_terms):\n",
        "    temp_d = {}\n",
        "    for tk in query_terms:\n",
        "      temp_d[tk] = self.tf_idf[tk]\n",
        "    return temp_d\n",
        "\n",
        "  def retrieval(self, query, k):\n",
        "    self.load_index(self.index_file)\n",
        "    # diccionario para el score\n",
        "    score = {}\n",
        "    # extraer los terminos unicos del query\n",
        "    query_terms = self.procesamiento(query)\n",
        "    # calcular el tf-idf del query\n",
        "    tfidf_query = self.get_tfidf(query_terms)\n",
        "    for term in query_terms:\n",
        "      list_pub = self.tf[term]\n",
        "      idf = self.idf[term]\n",
        "      for docid in list_pub:\n",
        "        if docid not in score:\n",
        "          score[docid] = 0\n",
        "        tfidf_doc = list_pub[docid] * idf\n",
        "        score[docid] += tfidf_query[term][docid] * tfidf_doc\n",
        "    # aplicar la norma \n",
        "    for docid in self.length:\n",
        "      score[docid] = score[docid] / self.length[docid]\n",
        "    \n",
        "    # convertir el diccionaro score a una lista [(doc1, score1), (doc2, score2), ...]\n",
        "    # ordenar respecto al score de forma descendente\n",
        "    result = sorted(score.items(), key= lambda tup: tup[1], reverse=True)\n",
        "    # retornamos los k documentos mas relevantes (de mayor similitud al query)\n",
        "    return result[:k]    \n",
        "\n",
        "  def load_index(self):\n",
        "    f = open(self.archivo_indice+\"_index.json\")\n",
        "    self.index = json.load(f)\n",
        "    f.close()"
      ],
      "metadata": {
        "id": "T6H6i5MWs1ps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "indice = indice_invertido(\"NEWS\")\n",
        "# indice.building(df.loc[:500])\n",
        "indice.building(df.loc[:500])"
      ],
      "metadata": {
        "id": "_h1QOQPFbD2U"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}